{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated feature detection and classification of Electrocardiographic Time Series data - Notebook Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1 - Library imports\n",
    "\n",
    "This cell will import all the libraries necessary for the machine learning pipeline. More libraries will be imported throughout the project for specific tasks but these libraries will be used for many of the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "import pywt\n",
    "from pywt import wavedec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2 - Empty DataFrame\n",
    "\n",
    "An empty pandas dataframe will be created, using the input columns which would be filled using the ECG database [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = pd.DataFrame(columns=['condition', 'name', 'ecg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3 - Data Upload\n",
    "\n",
    "A Python module 'FileWizard' has been written that will upload all the data to the empty dataframe.\n",
    "Database details:\n",
    "\n",
    "The data contains 1000 ECG fragments of sample size 3600, there are 17 different heart rhythms in the dataset\n",
    "\n",
    "Conditions:\n",
    "\n",
    "* 1. NSR, Normal Sinus Rhythm [283 items]\n",
    "* 2. APB, Atrial Premature Beat [66 items]\n",
    "* 3. AFL, Atrial Flutter [20 items]\n",
    "* 4. AFIB, Atrial Fibrilation [135 items]\n",
    "* 5. SVTA, Superventricular tachycardia (abnormally fast heart beat above ventricular area) [13 items]\n",
    "* 6. WPW, Wolf-Parkinson White Syndrome [21 items]\n",
    "* 7. PVC, Premature Ventricular Contraction [133 items]\n",
    "* 8. Bigeminy [55 items]\n",
    "* 9. Trigeminy [13 items]\n",
    "* 10. VT, Ventricular Tachycardia [10 items]\n",
    "* 11. IVR, Idioventricular Rhythm [10 items]\n",
    "* 12. VFL, Ventricular Flutter [10 items]\n",
    "* 13. Fusion [11 items]\n",
    "* 14. LBBB, Left Bundle Branch Block [103 items]\n",
    "* 15. RBBB, Right Bundle Branch Block [62 items]\n",
    "* 16. SDHB, Succinate dehydrogenase mutation [10 items]\n",
    "* 17. PR, Pulmonary Valve Reguritation [45 items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libraries.io import FileWizard\n",
    "\n",
    "path1 = 'E:/uni work/FYP/code/database/MLII/'\n",
    "\n",
    "fw = FileWizard()\n",
    "database = fw.start(path1, database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cells 4, 5 and 6 - Database testing status\n",
    "\n",
    "Database records will be checked to see if all data has been uploaded, in the correct columns, and that all the conditions are correctly present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(database.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Records in database = ' + str(database['name'].size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(database['condition'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7 - ECG exploration\n",
    "\n",
    "A healthy ECG heartbeat sample from the data is selected and explored. The SciPy library function find_peaks can be used to find the local maxima and minima, to find P, Q, R, S and T. This will form the basis of feature extraction later on in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = '1 NSR'\n",
    "examples = database[database['condition'] == case]\n",
    "example = examples.iloc[1]\n",
    "example_ecg = example['ecg']\n",
    "eecg = example_ecg[160:455]\n",
    "peaks, _ = find_peaks(eecg, height=960)\n",
    "lowpeaks, _ = find_peaks(-eecg, height=-910)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Normal Sinus Rhythm Signal\")\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"Electrical Activity\")\n",
    "plt.plot(example_ecg[160:455])\n",
    "plt.plot(peaks[2], eecg[peaks[2]], 'x')\n",
    "plt.plot(peaks[4], eecg[peaks[4]], 'x')\n",
    "plt.plot(lowpeaks[0], eecg[lowpeaks[0]], 'x')\n",
    "plt.plot(lowpeaks[1], eecg[lowpeaks[1]], 'x')\n",
    "plt.plot(peaks[5], eecg[peaks[5]], 'x')\n",
    "plt.annotate('P = (' + str(peaks[2],) + ', ' + str(eecg[peaks[2]]) + ')', xy =(peaks[2] - 10, eecg[peaks[2]] + 10))\n",
    "plt.annotate('Q = (' + str(lowpeaks[0],) + ', ' + str(eecg[lowpeaks[0]]) + ')', xy =(lowpeaks[0] - 50, eecg[lowpeaks[0]]))\n",
    "plt.annotate('R = (' + str(peaks[4],) + ', ' + str(eecg[peaks[4]]) + ')', xy =(peaks[4] - 50, eecg[peaks[4]]))\n",
    "plt.annotate('S = (' + str(lowpeaks[1],) + ', ' + str(eecg[lowpeaks[1]]) + ')', xy =(lowpeaks[1] + 5, eecg[lowpeaks[1]]))\n",
    "plt.annotate('T = (' + str(peaks[4],) + ', ' + str(eecg[peaks[5]]) + ')', xy =(peaks[5] - 20, eecg[peaks[5]] + 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8 - ECG comparison\n",
    "\n",
    "A brief ECG comparison between NSR and AFL. There are identifable temporal and amplitudinal differences in quality between the samples. The differences between temporal and amplitudinal quality can be measured and used as a feature of the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = '1 NSR'\n",
    "examples = database[database['condition'] == case]\n",
    "example = examples.iloc[1]\n",
    "example_ecg = example['ecg']\n",
    "\n",
    "case = '3 AFL'\n",
    "examples = database[database['condition'] == case]\n",
    "example = examples.iloc[1]\n",
    "example_ecg2 = example['ecg']\n",
    "\n",
    "fig, ((ax1, ax2)) = plt.subplots(2, figsize = (10,8), sharex=True, sharey=True)\n",
    "\n",
    "ax1.title.set_text(\"Normal Sinus Rhythm\")\n",
    "ax1.set_ylabel(\"Electrical Activity\")\n",
    "ax1.set_xlabel(\"Samples\")\n",
    "ax1.plot(example_ecg[0:1200])\n",
    "ax2.title.set_text(\"Atrial Flutter\")\n",
    "ax2.set_ylabel(\"Electrical Activity\")\n",
    "ax2.set_xlabel(\"Samples\")\n",
    "ax2.plot(example_ecg2[0:1200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9 - Multi ECG plotting method\n",
    "\n",
    "Method used for plotting a selected piece of data from an ECG record or database. The method takes in the database, the condition, title desired for the plot, the record index desired, and what part of the record should be plotted. This method will be essential for multiple ECG data comparison. The method then queries the database for ECGs where the condition equals the input \"case\", then using the result of this query, record \"i\" would be obtained. Then to get a specific data sample from that record, the data from the desired column is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_example(case, data, title, i, column):\n",
    "    examples = data[data['condition'] == case]\n",
    "    example = examples.iloc[i]\n",
    "    example_ecg = example[column]\n",
    "    \n",
    "    plt.figure(figsize=(15,4))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Samples\")\n",
    "    plt.ylabel(\"Electrical Activity\")\n",
    "    plt.plot(example_ecg)\n",
    "    plt.show()\n",
    "    \n",
    "lists = ['1 NSR', '2 APB', '3 AFL', '4 AFIB', '5 SVTA', '6 WPW', \n",
    "         '7 PVC', '8 Bigeminy', '9 Trigeminy', '10 VT', '11 IVR', \n",
    "         '12 VFL', '13 Fusion', '14 LBBBB', '15 RBBBB', '16 SDHB', '17 PR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10 - Multi ECG comparison\n",
    "\n",
    "Using the method, a for loop will plot all ECGs for heart condition classes. For every case record 1 of every case will be plotted. This will be used to view the full dataset and to expect what the heart conditions would typically look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in lists:\n",
    "    plot_example(item, database, (item + ' as raw data'), 1, 'ecg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11 - Noise Removal\n",
    "\n",
    "A simplified method to remove baseline noise artefacts in ECGS [2] was devised by Balasubramanian, S. V. This method was used to remove baseline noise, and to re-scale the ECGs so the values are normalized. Also a Savitzky Golay filter was simultaneously used to remove phase noise from the ECGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libraries.noise_removal import BaselineNoiseRemover\n",
    "\n",
    "# DC Notch filter to remove baseline noise from all signals\n",
    "\n",
    "bnr = BaselineNoiseRemover(c = -0.99)\n",
    "\n",
    "ecg_waves = database['ecg'].tolist()\n",
    "ecg_filt = []\n",
    "\n",
    "for wave in ecg_waves:\n",
    "    filt = bnr.fit(wave)\n",
    "    ecg_filt.append(filt)\n",
    "\n",
    "database['ecg'] = pd.Series(ecg_filt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12 - Result of Noise Removal\n",
    "\n",
    "Using the same method as before, all records will be plotted to view the result of baseline noise removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in lists:\n",
    "    plot_example(item, database, (item + ' with baseline noise removed'), 0, 'ecg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 13 - Daubechies 4 Wavelet\n",
    "Demonstration of Daubechies 4 wavelet. This will be used for the Discrete Wavelet Transform because the wavelet looks most identical to the ECG wave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelet = pywt.Wavelet('db4')\n",
    "phi, psi, x = wavelet.wavefun(level=4)\n",
    "plt.plot(x, psi)\n",
    "plt.title('Daubechies 4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 14 - Frequency Separation of ECGs.\n",
    "\n",
    "In order to perform feature extraction effectively the PQRST components must be separated from each other. This is to prevent the threshold-based peak detection method from coming across any artefacts when locating the position of ECG components. The Frequency Extractor module uses DWT to decompose an ECG, and inverse DWT to recompose high frequency components of the ECG only, to retrieve the QRS complex frequencies. A novel step detection method also developed extracts P and T frequencies and stores them in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libraries.feature_extraction import FrequencyExtractor\n",
    "\n",
    "fe = FrequencyExtractor()\n",
    "database = fe.fit(database)\n",
    "# Multilevel discrete decomposition to extract large frequencies from time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cells 15, 16 and 17 - Frequency Separation Results and Removal of Low Quality Data\n",
    "\n",
    "A demonstration of the results of frequency extraction. Both large and small frequencies will be plotted here. The DWT method was successful in producing high quality QRS complex data ready for separation. However, because the quality of the P and T data is lacking, it is very difficult to extract notable features from the data.\n",
    "\n",
    "The two conditions 10 VT and 12 VFL contain extremely noisy data in their ECGs, and thus have no real visible interval, so they will be ruled out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in lists:\n",
    "    plot_example(item, database, (item + ' large frequencies'), 10, 'large frequencies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in lists:\n",
    "    plot_example(item, database, (item + ' small frequencies'), 1, 'small frequencies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = ['10 VT', '12 VFL']\n",
    "\n",
    "for item in remove:\n",
    "    database = database[database['condition'] != item]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 18 - Distribution Comparison\n",
    "\n",
    "To create as much feature data as possible for optimal machine learning results, there should be feature data for every heartbeat. A method will now be explored and designed to see what way the heartbeats can be separated. First of all, the distributions between two different QRS samples of the same condition are compared. As observable, they are out of phase with each other, which means a fixed window to separate the heartbeats would not be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "examples = database[database['condition'] == '1 NSR']\n",
    "examples2 = database[database['condition'] == '1 NSR']\n",
    "example1 = examples.iloc[1]\n",
    "example2 = examples2.iloc[2]\n",
    "\n",
    "ecg1 = example1['large frequencies']\n",
    "ecg2 = example2['large frequencies']\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.plot(ecg1)\n",
    "plt.plot(ecg2)\n",
    "plt.title('Comparison of distributions between two Normal Sinus Rhythm waves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 19 - Dynamic Time Warping\n",
    "\n",
    "A well known algorithm called Dynamic Time Warping has been used often to measure the difference in distribution between two time series data, I have developed my own implementation of the algorithm and it will be used to compare how similar in physical alignment the two ECGs above are. A distance matrix is calculated where the manhattan distance between each point of the two arrays are calculated, and then using the distance matrix, an optimal path will be found between the ECGs. If the path on the matrix is a straight line, that means that the speed of both signals are constant, and a fixed window can be used to separate the heartbeats. If not then a method must be developed that can separate heartbeats of various distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libraries.distributions import DynamicTimeWarper\n",
    "\n",
    "dtw = DynamicTimeWarper()\n",
    "\n",
    "cost_matrix, path = dtw.fit(ecg1, ecg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 20 - Distance Matrix\n",
    "\n",
    "As observable, the optimal pathway between the two samples is not directly proportional, and so a method must be developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "ax = plt.axes()\n",
    "sns.heatmap(cost_matrix, cmap='coolwarm_r', ax=ax)\n",
    "ax.set_title(\"Dynamic Time Warping\")\n",
    "ax.set(xlabel='ECG 1', ylabel='ECG 2')\n",
    "\n",
    "positions = path[1]\n",
    "a = positions[:,1]\n",
    "b = positions[:,0]\n",
    "ax.plot(a + 0.5, b + 0.5, color='blue', linewidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 21 - Peak Extraction\n",
    "\n",
    "A method to extract heartbeats from ECGs is to find the most prominent peaks, which are R peaks, then find the midpoint between every R peak which will be the separation point of the heartbeat. For conditions with a less prominent QRS complex a lower threshold is used.\n",
    "\n",
    "This cell will extract the R peaks of every ECG using the SciPy find_peaks function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libraries.feature_extraction import PeakExtractor\n",
    "\n",
    "high_thresh = ['15 RBBBB', '17 PR']\n",
    "\n",
    "db_low_thresh = database[database['condition'] == '11 IVR']\n",
    "database = database[database['condition'] != '11 IVR']\n",
    "\n",
    "thresh = 40\n",
    "prominence = 50\n",
    "pe = PeakExtractor(c=thresh, p=prominence)\n",
    "database = pe.fit(database, 'large frequencies', 'peaks', 'peak position')\n",
    "\n",
    "# Special threshold for 11 IVR signals, QRS complex is very small\n",
    "\n",
    "thresh = 10\n",
    "prominence = 25\n",
    "pe = PeakExtractor(c=thresh, p=prominence)\n",
    "db_low_thresh = pe.fit(db_low_thresh, 'large frequencies', 'peaks', 'peak position')\n",
    "\n",
    "db_high_thresh = database[database['condition'].isin(high_thresh)]\n",
    "database = database[~database['condition'].isin(high_thresh)]\n",
    "\n",
    "thresh = 80\n",
    "prominence = 50\n",
    "pe = PeakExtractor(c=thresh, p=prominence)\n",
    "db_high_thresh = pe.fit(db_high_thresh, 'large frequencies', 'peaks', 'peak position')\n",
    "\n",
    "frames = [database, db_low_thresh, db_high_thresh]\n",
    "database = pd.concat(frames)\n",
    "database = database.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 22 - Peak Extraction Test\n",
    "\n",
    "This is a test to ensure that peaks were extracted properly. The database will be queried for a record, and the ECG, along with its peaks will be plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = database[database['condition'] == '17 PR']\n",
    "example1 = examples.iloc[0]\n",
    "peaks1 = example1['peaks']\n",
    "ecg1 = example1['large frequencies']\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.title('QRS peak detection of NSR')\n",
    "plt.plot(ecg1)\n",
    "plt.plot(peaks1, ecg1[peaks1], \"x\")\n",
    "plt.plot(np.ones_like(ecg1) * thresh, \"--\", color=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(peaks1)\n",
    "print(ecg1[peaks1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cells 23 and 24 - Midpoint Extraction\n",
    "\n",
    "Now the peaks are found for every ECG, the midpoint between every peak is calculated. The same record will be queried again from the database, this time to find midpoints of that ECG. This is how the heartbeats will be separated by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libraries.feature_extraction import MidPointExtractor\n",
    "\n",
    "mpe = MidPointExtractor()\n",
    "database = mpe.fit(database, 'peaks', 'peak position', 'midpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = database[database['condition'] == '17 PR']\n",
    "example1 = examples.iloc[0]\n",
    "mid1 = example1['midpoints']\n",
    "mid1 = mid1[0,:]\n",
    "\n",
    "# Retrieval of the midpoints from the database record\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.title('Optimal window detection of QRS complex via midpoint calculation')\n",
    "plt.plot(ecg1)\n",
    "for xv in mid1:\n",
    "    plt.axvline(x=xv, color='black', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 25 and 26 - Test on P and T\n",
    "\n",
    "The test of the applicability of the wave separation method can be applied to the P and T waves, however the data quality produced was too low quality to extract features from. Unlike the QRS complexes which had consistent identifiable areas where features can be extracted from, these frequencies do not have enough identifiable characteristics or patterns that can be detected through simple thresholding. There are even some cases where some unremoved severe baseline noise remains in the data, which causes the P and T waves to rise above or below the threshold for peak detection, and then results in false positive peak detection or false negative peak detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 5\n",
    "prominence=0\n",
    "pe = PeakExtractor(c=thresh, p=prominence)\n",
    "database = pe.fit(database, 'small frequencies', 'small peaks', 'small peak position')\n",
    "\n",
    "examples = database[database['condition'] == '2 APB']\n",
    "example1 = examples.iloc[17]\n",
    "peaks2 = example1['small peaks']\n",
    "pt1 = example1['small frequencies']\n",
    "\n",
    "# Retrieval of database for the peaks and their positions\n",
    "\n",
    "plt.figure(figsize=(15,2))\n",
    "plt.title('P/T peak detection of NSR signal')\n",
    "plt.plot(pt1)\n",
    "plt.plot(peaks2, pt1[peaks2], \"x\")\n",
    "plt.plot(np.ones_like(pt1) * thresh, \"--\", color=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = mpe.fit(database, 'small peaks', 'small peak position', 'small midpoints')\n",
    "\n",
    "examples = database[database['condition'] == '2 APB']\n",
    "\n",
    "example1 = examples.iloc[17]\n",
    "mid2 = example1['small midpoints']\n",
    "mid2 = mid2[0,:]\n",
    "\n",
    "# Retrieval of the midpoints from the database record\n",
    "\n",
    "plt.figure(figsize=(15,2))\n",
    "plt.title('Optimal window detection of P and T signal via midpoint calculation')\n",
    "plt.plot(pt1)\n",
    "for xv in mid2:\n",
    "    plt.axvline(x=xv, color='black', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 27 - Heartbeat Separation\n",
    "\n",
    "Once midpoints are found in every QRS sample, they are separated into heartbeats, so that they can have features extracted. These separate heartbeats are then inserted into a new database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libraries.feature_extraction import WaveletSeparator\n",
    "\n",
    "qrs_db = pd.DataFrame(columns=['wavelet', 'condition', 'partof'])\n",
    "\n",
    "ws = WaveletSeparator()\n",
    "qrs_db = ws.fit(database, qrs_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cells 28 and 29 - Heartbeat Separation Results\n",
    "\n",
    "To test if the heartbeats were correctly separated, an ECG sample is queried from the first database, then from the second database, the first heartbeat from the sample is compared to the whole sample to see if the wave is correctly aligning with each other. This will also be tested for another condition, Atrial Flutter to see if the task is able to do the same for other conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2, ax3)) = plt.subplots(3, figsize = (10,8), sharex=True, sharey=True)\n",
    "\n",
    "examples = database[database['condition'] == '1 NSR']\n",
    "examples1 = examples.iloc[0]\n",
    "ecg1 = examples1['large frequencies']\n",
    "\n",
    "examples1r = qrs_db[qrs_db['condition'] == '1 NSR']\n",
    "examples1r = examples1r.iloc[0]\n",
    "wav = examples1r['wavelet']\n",
    "\n",
    "ax1.title.set_text(\"101m (0) NSR sample from database\")\n",
    "ax1.set_ylabel(\"Electrical Activity\")\n",
    "ax1.plot(ecg1)\n",
    "ax2.title.set_text(\"101m (0) heartbeat from new database\")\n",
    "ax2.set_ylabel(\"Electrical Activity\")\n",
    "ax2.plot(wav)\n",
    "ax3.title.set_text(\"Comparison\")\n",
    "ax3.plot(ecg1)\n",
    "ax3.plot(wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples1 = qrs_db[qrs_db['condition'] == '5 SVTA']\n",
    "examples2 = database[database['condition'] == '5 SVTA']\n",
    "\n",
    "examples1r = examples1.iloc[0]\n",
    "examples2r = examples2.iloc[0]\n",
    "\n",
    "wav2 = examples1r['wavelet']\n",
    "ecg2 = examples2r['large frequencies']\n",
    "\n",
    "fig, ((ax1, ax2, ax3)) = plt.subplots(3, figsize = (10,8), sharex=True, sharey=True)\n",
    "\n",
    "ax1.title.set_text(\"202m (1) Atrial Flutter Rhythm from database\")\n",
    "ax1.set_ylabel(\"Electrical Activity\")\n",
    "ax1.plot(ecg2)\n",
    "ax2.title.set_text(\"202m (1) Atrial Flutter heartbeat from new database\")\n",
    "ax2.plot(wav2)\n",
    "ax3.set_xlabel(\"Samples\")\n",
    "ax3.title.set_text(\"Comparison\")\n",
    "ax3.plot(ecg2)\n",
    "ax3.plot(wav2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 30 - Full Heartbeat Exploration\n",
    "\n",
    "The piece of code below can be used to explore the full results of the heartbeat separation task. Most heartbeats have been separated correctly, because the use of peak prominence and threshold adjustment can be manually adjusted depending on the condition of the ECG. The \"examples_iloc\" index value can be changed to any desired index number to view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_temp = ['1 NSR', '2 APB', '3 AFL', '4 AFIB', '5 SVTA', '6 WPW', \n",
    "         '7 PVC', '8 Bigeminy', '9 Trigeminy', '11 IVR', '13 Fusion', '14 LBBBB', '15 RBBBB', '16 SDHB', '17 PR']\n",
    "\n",
    "for item in list_temp:\n",
    "    examples = qrs_db[qrs_db['condition'] == item]\n",
    "    ewav = examples.iloc[10]\n",
    "    wav = ewav['wavelet']\n",
    "    plt.title(\"Single wave of \" + item)\n",
    "    plt.ylabel(\"Electrical Activity\")\n",
    "    plt.plot(wav)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cells 31 and 32 - Feature Extraction Design\n",
    "\n",
    "These graphical illustrations take an example of the heartbeat and then is used to show what type of features are extracted from this heartbeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples1 = qrs_db[qrs_db['condition'] == '1 NSR']\n",
    "ewav1 = examples1.iloc[1]\n",
    "wav1 = ewav1['wavelet']\n",
    "\n",
    "p1, pos1 = find_peaks(wav1, 25)\n",
    "p2, pos2 = find_peaks(-wav1, 25)\n",
    "print(wav1[p1])\n",
    "pos1 = pos1['peak_heights']\n",
    "pos2 = pos2['peak_heights']\n",
    "\n",
    "\n",
    "p3 = p2[1]\n",
    "p2 = p2[0]\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.title('Anatomy of the QRS (NSR Example)')\n",
    "plt.plot(wav1)\n",
    "plt.plot(p1, wav1[p1], 'x', color='black')\n",
    "plt.plot(p2, wav1[p2], 'x', color='black')\n",
    "plt.plot(p3, wav1[p3], 'x', color='black')\n",
    "w22 = -wav1[p2]\n",
    "print(wav1[p1])\n",
    "print(p1)\n",
    "print(w22)\n",
    "print(wav1[p1] + w22)\n",
    "\n",
    "string1 = ('coordinates Q: ' + str(p1) + ', ' + str(wav1[p1]))\n",
    "string2 = ('coordinates R: ' + str(p2) + ', ' + str(wav1[p2]))\n",
    "string3 = ('coordinates S: ' + str(p3) + ', ' + str(wav1[p3]))\n",
    "print(string1)\n",
    "print(string2)\n",
    "print(string3)\n",
    "plt.annotate(('S = (147, -67.5)'), xy =(p3 + 5, wav1[p3]))\n",
    "plt.annotate('Q = (127, -56.7)', xy =(p2 - 20, wav1[p2] - 10))\n",
    "plt.annotate('R = (135, 81.1)', xy =(p1 + 5, wav1[p1] - 8))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.title('Height between Q and R')\n",
    "plt.plot(wav1)\n",
    "plt.plot(p1, wav1[p1], 'x', color='black')\n",
    "plt.plot(p2, wav1[p2], 'x', color='black')\n",
    "w22 = -wav1[p2]\n",
    "print(wav1[p1])\n",
    "print(w22)\n",
    "print(wav1[p1] + w22)\n",
    "plt.axhline(wav1[p1], xmin=0.43, xmax=0.57, color='grey', linestyle='--')\n",
    "plt.axhline(wav1[p2], xmin=0.43, xmax=0.57, color='grey', linestyle='--')\n",
    "plt.axvline(130, ymin=0.1, ymax=0.95, color='grey', linestyle='--')\n",
    "plt.annotate('g = 137.8', xy =(70, 50))\n",
    "plt.annotate('Q = (127, -56.7)', xy =(p2 - 20, wav1[p2] - 10))\n",
    "plt.annotate('R = (135, 81.1)', xy =(p1 + 3, wav1[p1] - 8))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.title('Height between R to S')\n",
    "plt.plot(wav1)\n",
    "plt.plot(p1, wav1[p1], 'x', color='black')\n",
    "plt.plot(p3, wav1[p3], 'x', color='black')\n",
    "w22 = -wav1[p3]\n",
    "print(wav1[p1])\n",
    "print(w22)\n",
    "print(wav1[p1] + w22)\n",
    "plt.axhline(wav1[p1], xmin=0.43, xmax=0.57, color='grey', linestyle='--')\n",
    "plt.axhline(wav1[p3], xmin=0.43, xmax=0.57, color='grey', linestyle='--')\n",
    "plt.axvline(130, ymin=0.05, ymax=0.95, color='grey', linestyle='--')\n",
    "plt.annotate('g = (81.1 - -67.5) = 148.6', xy =(70, 50))\n",
    "plt.annotate('S = (147, -67.5)', xy =(p3 + 8, wav1[p3] - 1))\n",
    "plt.annotate('R = (135, 81.1)', xy =(p1 + 3, wav1[p1] - 8))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.title('Time gap between Q and S')\n",
    "plt.plot(p2, wav1[p2], 'x', color='black')\n",
    "plt.plot(p3, wav1[p3], 'x', color='black')\n",
    "plt.plot(wav1)\n",
    "plt.axhline(0, xmin=0.46, xmax=0.54, color='grey', linestyle='--')\n",
    "plt.axvline(p2, ymin=0.05, ymax=0.95, color='grey', linestyle='--')\n",
    "plt.axvline(p3, ymin=0.05, ymax=0.95, color='grey', linestyle='--')\n",
    "plt.annotate('Time length = 0.05 seconds', xy =(60, 50))\n",
    "plt.annotate('S = (147, -67.5)', xy =(p3 + 8, wav1[p3] - 1))\n",
    "plt.annotate('Q = (127, -56.7)', xy =(p2 - 20, wav1[p2] - 10))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.title('Time Length')\n",
    "plt.plot(wav1)\n",
    "w22 = -wav1[p3]\n",
    "print(wav1[p1])\n",
    "print(w22)\n",
    "print(wav1[p1] + w22)\n",
    "plt.axhline(0, xmin=0.05, xmax=0.95, color='grey', linestyle='--')\n",
    "plt.axvline(0, ymin=0.05, ymax=0.95, color='grey', linestyle='--')\n",
    "plt.axvline(len(wav1), ymin=0.05, ymax=0.95, color='grey', linestyle='--')\n",
    "plt.annotate('Time length = 0.7 seconds', xy =(90, 50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav1 = examples1.iloc[31]\n",
    "wav1w = wav1['wavelet']\n",
    "wav11 = examples1.iloc[32]\n",
    "wav11w = wav11['wavelet']\n",
    "demo = np.hstack((wav1w, wav11w))\n",
    "high, pos1 = find_peaks(demo, height=50)\n",
    "low, pos2 = find_peaks(-demo, height=30, prominence=6)\n",
    "\n",
    "r1 = high[0]\n",
    "r2 = high[1]\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.title('D1 and D2')\n",
    "plt.plot(demo)\n",
    "plt.show()\n",
    "\n",
    "print(r1)\n",
    "print(demo[r1])\n",
    "print(r2)\n",
    "print(demo[r2])\n",
    "\n",
    "print((r2- r1) * 0.0027777777)\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.title('Time gap between two intervals')\n",
    "plt.plot(demo)\n",
    "plt.plot(r1, demo[r1], 'x', color='black')\n",
    "plt.plot(r2, demo[r2], 'x', color='black')\n",
    "plt.annotate('Time gap length = 0.71 seconds', xy =(200, 50))\n",
    "plt.annotate('R1 = (128, 82.9)', xy =(r1 - 65, demo[r1] - 5))\n",
    "plt.annotate('R2 = (383, 88.3)', xy =(r2 - 65, demo[r2] - 5))\n",
    "plt.axvline(r1, ymin=0.05, ymax=0.95, color='black', linestyle='--')\n",
    "plt.axvline(r2, ymin=0.05, ymax=0.95, color='black', linestyle='--')\n",
    "plt.axhline(0, xmin=0.28, xmax=0.71, color='black', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cells 33, 34, 35, and 36 - Full Feature Extraction\n",
    "\n",
    "These features are then extracted from the entire dataset of heartbeats. A module that I have developed in Python uses the same QRS detection techniques to fill the database with scalar numbered features relating to the amplitudinal and temporal composition of the heartbeat. Once this feature extraction process has completed, then in the case of any NaN values due to anomalous data having an effect on feature extraction, this can be removed using \"dropna()\".\n",
    "\n",
    "A backup database called \"qrs_db_backup\" has been created in the event of data corruption that is based on the original database. Run Cell 34 in case of this happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrs_db_backup = qrs_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrs_db = qrs_db_backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libraries.qrs_features import QRSfeatures\n",
    "\n",
    "qrsf = QRSfeatures(50)\n",
    "qrs_db= qrsf.fit(qrs_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrs_db = qrs_db.dropna()\n",
    "qrs_db = qrs_db.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cells 37 and 38 - Box Plot of Features\n",
    "\n",
    "This is the results of the feature extraction, from the boxplots of the five features, the distributions of the five features can be viewed here. The significance of the box plot is to see how distinct the feature values make the conditions when it comes to classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listdata = ['1 NSR', '2 APB', '3 AFL', '4 AFIB', '5 SVTA', '6 WPW', \n",
    "         '7 PVC', '8 Bigeminy', '9 Trigeminy', '11 IVR', '13 Fusion', \n",
    "            '14 LBBBB', '15 RBBBB', '16 SDHB', '17 PR']\n",
    "\n",
    "def box(data, label, time_mode):\n",
    "    \n",
    "    list3 = []\n",
    "    \n",
    "    for item in listdata:  \n",
    "        subject = data[data['condition'] == item]\n",
    "        lots = subject[label].to_list()\n",
    "        col = pd.DataFrame(lots, columns=[item])\n",
    "        list3.append(col)\n",
    "    \n",
    "    new_data = pd.concat(list3, axis=1)\n",
    "    bp = new_data.boxplot(column=listdata, figsize=((15,10)), vert=False)\n",
    "    \n",
    "    if time_mode == True:\n",
    "        bp.set_xlabel(label + ' (seconds)') \n",
    "    else:\n",
    "        bp.set_xlabel(label + ' (mV)') \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box(qrs_db[['condition','time length']], 'time length', True)\n",
    "box(qrs_db[['condition','qr height']], 'qr height', False)\n",
    "box(qrs_db[['condition','rs height']], 'rs height', False)\n",
    "box(qrs_db[['condition','qs time length']], 'qs time length', True)\n",
    "box(qrs_db[['condition','time gap']], 'time gap', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cells 39, 40 and 41 - Removal of Outliers\n",
    "\n",
    "Because it is very likely that outliers will be found in the data, then there needs to be a function that can remove values that are beyond the whiskers of every condition and the interquartile range. If outliers are not removed this can severely cause the feature space to become fuzzy, therefore hindering the accuracy of the machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(data):\n",
    "    \n",
    "    variables = ['time gap', \n",
    "             'qr height', 'rs height', 'qs time length', 'time length']\n",
    "\n",
    "    for var in variables:\n",
    "        cols = ['time gap', 'qr height', 'rs height', 'qs time length', 'time length']\n",
    "        Q1 = data[cols].quantile(0.25)\n",
    "        Q3 = data[cols].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        data = data[~((data[cols] < (Q1 - 1.5 * IQR)) |(data[cols] > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrs_db_t = qrs_db\n",
    "\n",
    "new_subjects = []\n",
    "\n",
    "for cond in listdata:\n",
    "    subject = qrs_db_t[qrs_db_t['condition'] == cond]\n",
    "    new_subject = remove_outliers(subject)\n",
    "    new_subjects.append(new_subject)\n",
    "    \n",
    "qrs_db_t = pd.concat(new_subjects)\n",
    "qrs_db_t = qrs_db_t.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box(qrs_db_t[['condition','time length']], 'time length', True)\n",
    "box(qrs_db_t[['condition','qr height']], 'qr height', False)\n",
    "box(qrs_db_t[['condition','rs height']], 'rs height', False)\n",
    "box(qrs_db_t[['condition','qs time length']], 'qs time length', True)\n",
    "box(qrs_db_t[['condition','time gap']], 'time gap', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 42 - Data Equalization\n",
    "\n",
    "For optimal results, it is best that for every case the number of heartbeats are equalized. The database is edited so that for every case in the data the first 60 heartbeats are taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_data(data, listdata, maximum):  \n",
    "    lists2 = []\n",
    "    for item in listdata:  \n",
    "        subject = data[data['condition'] == item]\n",
    "        subject = subject[0:maximum]\n",
    "        lists2.append(subject)\n",
    "    \n",
    "    new_data = pd.concat(lists2)\n",
    "    new_data = new_data.reset_index(drop=True)\n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrs_db_t = edit_data(qrs_db_t, listdata, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cells 43 and 44 - Linear Analysis of Features\n",
    "\n",
    "To study how strong of a relationship two features have, a linear analysis of two variables can be undertaken. This measures how each condition is clustered and represented in the feature space. If a large majority of heart conditions cluster around a certain feature range on the two dimensional space, it shows that there can be a decision boundary established."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_analysis(data, labels, var1, var2):\n",
    "    plt.figure(figsize=(15,15))\n",
    "    for item in labels:\n",
    "        subject = data[data['condition'] == item]\n",
    "        plt.plot(subject[var1], subject[var2], 'x', label=str(item))\n",
    "    \n",
    "    plt.title('Feature space of heartbeats')\n",
    "    plt.xlabel(var1)\n",
    "    plt.ylabel(var2)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_analysis(qrs_db_t, listdata, 'time length', 'time gap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cells 46 and 47 - Test 1 preparation\n",
    "\n",
    "For the first machine learning test, only the two time related features, time length and time gap will be used. This is to test first of all how the machine learning performs with only one category of features, the temporal. If the machine learning model underperforms, then it is an indication that more features are necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_labels(X, y, label):\n",
    "    for i, item in enumerate(label):\n",
    "        y = y.replace(label, i)\n",
    "    y = y.astype(\"category\")\n",
    "    y = y.reset_index(drop='True')\n",
    "    X = X.reset_index(drop='True')\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = qrs_db_t[['time length', 'time gap']].astype(float)\n",
    "y = qrs_db_t['condition']\n",
    "\n",
    "X, y = parse_labels(X, y, listdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cells 48 and 49 - XGBoost Random Forest parameters\n",
    "\n",
    "This method will be responsible for arranging the XGBoost parameters for every model. This is so the parameters do not have to be changed constantly with every test. To ensure a repeatable and reproducible test all parameters should be the same for every test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def ml_model(X, y):\n",
    "    params = {\n",
    "        'colsample_bynode': 0.8,\n",
    "        'learning_rate': 3,\n",
    "        'max_depth': 6,\n",
    "        'num_parallel_tree': 300,\n",
    "        'subsample': 0.8,\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'reg_lambda': 100\n",
    "    }\n",
    "\n",
    "    xgb = XGBClassifier(params)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, \n",
    "                                                        test_size=0.3, stratify=y, random_state=0)\n",
    "    xgb.fit(X_train, y_train)\n",
    "    y_pred = xgb.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "    \n",
    "    return y_test, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 50 - Confusion Matrix\n",
    "\n",
    "For every test a confusion matrix will be plotted in order to analyse where the machine learning models went wrong. By seeing what the model misclassified, the quality of the features as an input can potentially be used to justify the areas of confusion that a machine learning model may experience. This method written here plots the confusion matrix for the predictions made by the machine learning model vs the actual results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def conmat(test, pred, labels):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    ax = plt.axes()\n",
    "    mat = confusion_matrix(test_one_test, test_one_pred)\n",
    "    sns.heatmap(mat, cmap='coolwarm_r', ax=ax, xticklabels=labels, yticklabels=labels, annot=True)\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 51 - Classification Report\n",
    "\n",
    "The classification report imported through sci-kit learn will be used to calculate the following for every model:\n",
    "\n",
    "* Precision - the proportion of correctly predicted positives in the model against all total predictions that were considered positive by the model\n",
    "* Recall - the proportion of correctly predicted positives in the model against the number of actual positive cases in the data\n",
    "* F1-Score - the weighted average of precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 52 - Test 1\n",
    "\n",
    "The running and results of the Test 1 model including confusion matrix and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_one_test, test_one_pred = ml_model(X, y)\n",
    "conmat(test_one_test, test_one_pred, listdata)\n",
    "print(classification_report(y_test, y_pred, target_names=listdata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cells 53 and 54 - Test 2\n",
    "\n",
    "Now a test using all of the features. This test will test the significance of using all of the data's features. If accuracy has increased, it shows that more features improve the decision boundaries of data prediction. However if accuracy decreases there may be an error in the data itself, or the choice of machine learning model is not suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = qrs_db_t[['time length', 'time gap', 'qr height', 'rs height', 'qs time length']].astype(float)\n",
    "y = qrs_db_t['condition']\n",
    "\n",
    "X, y = parse_labels(X, y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_one_test, test_one_pred = ml_model(X, y)\n",
    "conmat(test_one_test, test_one_pred, listdata)\n",
    "print(classification_report(y_test, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cells 55, 56, 57, 58, 59 and 60 - Test 3\n",
    "\n",
    "The final test will not use the features for training but will instead use the input of the raw wavelets into the model. This indicate if the extraction of features was actually necessary for the project. The expected result is that the prediction would be fuzzier than the previous tests, because arrays have a far bigger space complexity than scalar feature variables. Because the input matrix must be of two dimensions, all the heartbeats will need to be padded so that every heartbeat is the same array size, and then all heartbeat arrays will be stacked to form a two dimensional matrix. This does not affect the physical composition of the waves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = qrs_db_t['wavelet']\n",
    "XL = qrs_db_t['time length']\n",
    "\n",
    "y = qrs_db_t['condition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for a in X:\n",
    "    lengths.append(len(a))\n",
    "maximum = np.amax(lengths)\n",
    "print(maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = []\n",
    "for x in X:\n",
    "    x = np.resize(x, maximum)\n",
    "    X_new.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in X_new:\n",
    "    print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asmatrix(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_one_test, test_one_pred = ml_model(X, y)\n",
    "conmat(test_one_test, test_one_pred, listdata)\n",
    "print(classification_report(y_test, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. P lawiak, P. [2017], ‘Ecg signals (1000 fragments)’.\n",
    "2. Balasubramanian, S. V. [n.d.], ‘Power line interference and baseline removal in ecg’."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
